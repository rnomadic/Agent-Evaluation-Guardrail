import os
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevance, context_recall, context_precision
# NOTE: RAGAS often requires an LLM to judge quality.
# For demonstration, we assume OpenAI is configured, or you can configure a different model.
# To use OpenAI, set your environment variable: os.environ["OPENAI_API_KEY"] = "YOUR_API_KEY"
# Alternatively, configure LLMs for specific metrics, e.g.,
# from ragas.llms import RagasLLM
# from langchain_openai.chat_models import ChatOpenAI
# faithfulness.llm = RagasLLM(model=ChatOpenAI(model="gpt-4o"))

def run_ragas_evaluation():
    """
    Sets up a synthetic dataset and runs the RAGAS evaluation.
    """
    print("--- 1. Setting up Synthetic Evaluation Dataset ---")

    # A RAGAS dataset must contain these four columns:
    # 1. question: The user query.
    # 2. answer: The answer generated by your RAG pipeline.
    # 3. contexts: The chunks of text retrieved by your RAG pipeline.
    # 4. ground_truth: The correct, reference answer (optional for some metrics).

    data_samples = {
        'question': [
            "What is the capital of France?",
            "What is a key benefit of using vector databases?",
            "How does photosynthesis work?"
        ],
        'answer': [
            "The capital of France is Paris, famous for the Eiffel Tower.",
            "They help in efficient similarity searches for unstructured data.",
            "Photosynthesis is the process by which plants convert light energy into chemical energy."
        ],
        'contexts': [
            ["Paris is the largest city and capital of France, situated on the river Seine.", "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars."],
            ["Vector databases store data as high-dimensional vectors, enabling fast and accurate nearest neighbor search based on semantic similarity."],
            ["Plants use sunlight, water, and carbon dioxide to create glucose (sugar) and oxygen through photosynthesis, a vital process for life."]
        ],
        'ground_truth': [
            "Paris, the most populous city in France, is the official capital.",
            "Efficiently search for semantically similar data points, critical for RAG systems.",
            "Plants convert light into chemical energy (glucose) using water and CO2."
        ]
    }

    # Convert the dictionary to a Pandas DataFrame and then to a RAGAS Dataset
    df = pd.DataFrame(data_samples)
    dataset = Dataset.from_pandas(df)

    print(f"Dataset loaded with {len(dataset)} samples.")

    print("\n--- 2. Running RAGAS Evaluation ---")

    # Define the metrics we want to use
    metrics = [
        faithfulness,       # Measures how factual the generated answer is w.r.t the retrieved context.
        answer_relevance,   # Measures how relevant the generated answer is to the question.
        context_recall,     # Measures how well the retrieved context covers the information needed in the ground truth.
        context_precision   # Measures how relevant the retrieved context is to the question.
    ]

    # NOTE: You must have an LLM configured (e.g., OPENAI_API_KEY set) for this step to succeed.
    try:
        results = evaluate(
            dataset=dataset,
            metrics=metrics,
        )

        print("\n--- 3. Evaluation Results ---")
        # Print the overall scores
        print(results)

        # Print the results as a DataFrame for easy viewing
        results_df = results.to_pandas()
        print("\n--- Detailed Results (per sample) ---")
        print(results_df[['question', 'faithfulness', 'answer_relevance', 'context_recall', 'context_precision']].to_markdown(index=False))

    except Exception as e:
        print(f"\n!! RAGAS Evaluation Failed !!")
        print("Please ensure your LLM configuration (e.g., OPENAI_API_KEY) is correctly set up.")
        print(f"Error: {e}")

if __name__ == "__main__":
    run_ragas_evaluation()